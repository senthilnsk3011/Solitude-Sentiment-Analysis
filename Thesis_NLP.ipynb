{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca95236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NSK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "from sklearn import metrics\n",
    "\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from text_preprocessing import preprocess_text\n",
    "from text_preprocessing import to_lower, remove_punctuation, lemmatize_word,check_spelling,expand_contraction,keep_alpha_numeric,remove_special_character,remove_stopword,remove_whitespace,remove_number\n",
    "preprocessing_to_apply = [preprocess_text,to_lower, remove_punctuation, lemmatize_word,check_spelling,expand_contraction,keep_alpha_numeric,remove_special_character,remove_stopword,remove_whitespace] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1872dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"HARP_ThoughtSampling_TextData_092022.csv\",encoding= 'latin-1')\n",
    "df=df[:4565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10833c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"(no description provided)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"[translation pending]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"(no recording)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"[no recording]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"[no entry]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_1_CodeTR02'] ==\"nothing\"].index, inplace = True)\n",
    "\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"(no description provided)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"[translation pending]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"(no recording)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"[no recording]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"[no entry]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_2_CodeTR02'] ==\"nothing\"].index, inplace = True)\n",
    "\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"(no description provided)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"[translation pending]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"(no recording)\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"[no recording]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"[no entry]\"].index, inplace = True)\n",
    "df.drop(df[df['ThoughtSamplTEXT_3_CodeTR02'] ==\"nothing\"].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3b6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ThoughtSamplMODE_1_CodeTR01'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1e2658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NSK\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.ThoughtSamplMODE_1_CodeTR01)\n",
    "plt.xlabel('Mode of Input');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e77e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['QuestionnaireDate', 'QuestionnaireTime','ThoughtSamplMODE_1_CodeTR01','ThoughtSampl_1_RecordingDate','ThoughtSampl_1_RecordingTime','ThoughtSampl_2_RecordingDate',\n",
    "                'ThoughtSamplMODE_2_CodeTR01','ThoughtSampl_2_RecordingTime', 'ThoughtSamplMODE_3_CodeTR01','ThoughtSampl_3_RecordingDate',\n",
    "                'Unnamed: 88', 'Unnamed: 89','T01', 'T01_rt', 'T02', 'T02_rt', 'T03','T03_rt', 'T04', 'T04_rt', 'T05', 'T05_rt', 'T06', 'T06_rt', 'T07','T07_rt', 'T08', 'T08_rt', 'T22a', 'T22b', 'T22c',\n",
    "                'T22d', 'T22e', 'T22f', 'T22_rt', 'T23a', 'T23b', 'T23c', 'T23d','T23e', 'T23f', 'T23g', 'T23h', 'T23_rt', 'T24a', 'T24b', 'T24c','T24d', 'T24e', 'T24f', 'T24g', 'T24h', 'T24_rt', 'T25', \n",
    "                 'T25_rt','T26', 'T26_rt','T09_rt','T10_rt','T11_rt','T12_rt','T13_rt','T14_rt','T15_rt','T16_rt','T17_rt','T18_rt','T19_rt','T20_rt','T21_rt','StudyDay','ThoughtSampl_3_RecordingTime',\n",
    "                 'T18','T11','T20','T19','T17','T13','T16','T12' ,'T14' ,'T21', 'ParticipantID' , 'Language'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c240cc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThoughtSamplTEXT_1_CodeTR02       1\n",
       "ThoughtSamplTEXT_2_CodeTR02    4013\n",
       "ThoughtSamplTEXT_3_CodeTR02    4068\n",
       "T09                               6\n",
       "T10                               6\n",
       "T15                               6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a26419ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['T09'],inplace=True)\n",
    "df.dropna(subset=['T10'],inplace=True)\n",
    "df.dropna(subset=['T15'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b65085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['ThoughtSamplTEXT_1_CodeTR02'],inplace=True)\n",
    "df.rename(columns={'T09': 'HAPPY', 'T10': 'CALM', 'T15': 'SAD'}, inplace=True)\n",
    "\n",
    "df['ThoughtSamplTEXT_2_CodeTR02'].fillna(\" \", inplace=True)\n",
    "df['ThoughtSamplTEXT_3_CodeTR02'].fillna(\" \", inplace=True)\n",
    "\n",
    "df = df[['ThoughtSamplTEXT_1_CodeTR02','ThoughtSamplTEXT_2_CodeTR02',\n",
    "        'ThoughtSamplTEXT_3_CodeTR02','HAPPY','SAD','CALM']]\n",
    "\n",
    "df['HAPPY'] = df['HAPPY'].astype(int)\n",
    "df['SAD'] = df['SAD'].astype(int)\n",
    "df['CALM'] = df['CALM'].astype(int)\n",
    "\n",
    "conditions = [(df['HAPPY']>=df['SAD']) & (df['HAPPY']>=df['CALM']),\n",
    "             (df['SAD']>=df['HAPPY']) &(df['SAD']>=df['CALM']),\n",
    "             (df['CALM']>=df['HAPPY'])&(df['CALM']>=df['SAD'])]\n",
    "\n",
    "choices = [ '2','1','0']\n",
    "\n",
    "df['LABELS'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "df[\"ThoughtSamplTEXT_1_CodeTR02\"] = df[[\"ThoughtSamplTEXT_1_CodeTR02\", \"ThoughtSamplTEXT_3_CodeTR02\",'ThoughtSamplTEXT_3_CodeTR02' ]].apply(\"-\".join, axis=1)\n",
    "df.drop(columns=['ThoughtSamplTEXT_3_CodeTR02', \"ThoughtSamplTEXT_2_CodeTR02\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d679ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NSK\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.LABELS)\n",
    "plt.xlabel('Sentiment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b29ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['ThoughtSamplTEXT_1_CodeTR02'].str.lower()\n",
    "df.drop(columns = ['ThoughtSamplTEXT_1_CodeTR02'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3796d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461db7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... '}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73f654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['preprocessed_text'].apply(text_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80619277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "df['preprocessed_text'] = df[\"preprocessed_text\"].apply(lambda text:spell(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534323a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4064 entries, 2 to 4564\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   HAPPY              4064 non-null   int32 \n",
      " 1   SAD                4064 non-null   int32 \n",
      " 2   CALM               4064 non-null   int32 \n",
      " 3   LABELS             4064 non-null   int64 \n",
      " 4   preprocessed_text  4064 non-null   object\n",
      "dtypes: int32(3), int64(1), object(1)\n",
      "memory usage: 271.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df['LABELS'] = df['LABELS'].astype('int64')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a578521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3251, 2)\n",
      "(406, 2)\n",
      "(407, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3251 entries, 1718 to 4223\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             3251 non-null   int64 \n",
      " 1   preprocessed_text  3251 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 76.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 406 entries, 1011 to 3676\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             406 non-null    int64 \n",
      " 1   preprocessed_text  406 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 407 entries, 4433 to 4544\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             407 non-null    int64 \n",
      " 1   preprocessed_text  407 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = df.drop(columns=['HAPPY','SAD','CALM'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size=0.8\n",
    "train_set , train_rem= train_test_split(df1, train_size=0.8)\n",
    "\n",
    "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "test_size = 0.5\n",
    "valid_set, test_set   = train_test_split(train_rem, test_size=0.5)\n",
    "\n",
    "print(train_set.shape), \n",
    "print(valid_set.shape), \n",
    "print(test_set.shape), \n",
    "\n",
    "train_set.info()\n",
    "valid_set.info()\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b97a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3251 entries, 1718 to 4223\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             3251 non-null   int64 \n",
      " 1   preprocessed_text  3251 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 76.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 406 entries, 1011 to 3676\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             406 non-null    int64 \n",
      " 1   preprocessed_text  406 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 407 entries, 4433 to 4544\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   LABELS             407 non-null    int64 \n",
      " 1   preprocessed_text  407 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()\n",
    "valid_set.info()\n",
    "test_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86601ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24626b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\NSK\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train_set.preprocessed_text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    valid_set.preprocessed_text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(train_set.LABELS.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(valid_set.LABELS.values)\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b070985f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 2: 1, 1: 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.LABELS.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec3f2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Downloading Pre-trained BERT Uncased base Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=3,output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 3\n",
    "dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train),batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0441f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NSK\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=1e-5, eps=1e-8)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aaa7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19aab20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#setting up the cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23112f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model validation \n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17320192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6990800171b14310aafdbb393cbab528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.9802487900556234\n",
      "Validation loss: 0.9191004237269654\n",
      "F1 Score (Weighted): 0.3525830738530651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.8924137266607082\n",
      "Validation loss: 0.9627730512224576\n",
      "F1 Score (Weighted): 0.4392543272959893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.694099604045608\n",
      "Validation loss: 1.111636368530419\n",
      "F1 Score (Weighted): 0.49992155524526294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.5185320679110396\n",
      "Validation loss: 1.5778627061107031\n",
      "F1 Score (Weighted): 0.5266414841690151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/1084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.42392003497347663\n",
      "Validation loss: 1.7823689779068124\n",
      "F1 Score (Weighted): 0.5271298531545767\n"
     ]
    }
   ],
   "source": [
    "# Getting metrics for each epoch\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()  \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), 'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7f2d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5affc48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_{epoch}.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d42c125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57c7e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Accuracy: 127/196\n",
      "\n",
      "Class: 2\n",
      "Accuracy: 11/39\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 78/171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82893cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: [[1883   24   80]\n",
      " [ 459   25   13]\n",
      " [1486   17   77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.492     0.948     0.648      1987\n",
      "           1      0.379     0.050     0.089       497\n",
      "           2      0.453     0.049     0.088      1580\n",
      "\n",
      "    accuracy                          0.488      4064\n",
      "   macro avg      0.441     0.349     0.275      4064\n",
      "weighted avg      0.463     0.488     0.362      4064\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    \n",
    "    if (scores[\"pos\"] > scores[\"neg\"]) and (scores[\"pos\"] > scores[\"neu\"]):\n",
    "        return 2\n",
    "    elif scores[\"neg\"] > scores[\"pos\"] and (scores[\"neg\"] > scores[\"neu\"]):\n",
    "        return 1\n",
    "    return 0\n",
    "  \n",
    "df[\"vader_result\"] = df[\"preprocessed_text\"].apply(lambda x: vader_sentiment_result(x))\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(df[\"LABELS\"], df[\"vader_result\"])\n",
    "print(\"Confusion Matrix:\", matrix)\n",
    "\n",
    "print(metrics.classification_report(df[\"LABELS\"], df[\"vader_result\"], digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a43e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 0.738, 'pos': 0.262, 'compound': 0.9869}\n",
      "sentence was rated as  0.0 % Negative\n",
      "sentence was rated as  73.8 % Neutral\n",
      "sentence was rated as  26.200000000000003 % Positive\n",
      "Sentence Overall Rated As : "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CALM'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vader_sentiment_result_visualize(sent):\n",
    "    scores_vis = analyzer.polarity_scores(sent)\n",
    "    \n",
    "    print(\"Overall sentiment dictionary is : \", scores_vis)\n",
    "    print(\"sentence was rated as \", scores_vis['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", scores_vis['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", scores_vis['pos']*100, \"% Positive\")\n",
    "    print(\"Sentence Overall Rated As :\", end = \" \")\n",
    "    \n",
    "    if (scores_vis[\"pos\"] > scores_vis[\"neg\"]) and (scores_vis[\"pos\"] > scores_vis[\"neu\"]):\n",
    "        return \"HAPPY\"\n",
    "    elif scores_vis[\"neg\"] > scores_vis[\"pos\"] and (scores_vis[\"neg\"] > scores_vis[\"neu\"]):\n",
    "        return \"SAD\"\n",
    "    return \"CALM\"\n",
    "\n",
    "sent1 = df.iat[1,4]\n",
    "vader_sentiment_result_visualize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cefc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
